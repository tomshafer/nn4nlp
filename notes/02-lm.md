# Lecture 2 Notes and Resources

**Count Models and Smoothing (Slide 6)**

 - Related to Bayesian shrinkage
 - See https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf 
   for more details
   - Bullet 2 seems to be "Jelinek-Mercer smoothing" (Slide 16 of PDF) 

**Evaluation (Slide 7)**

 - Per-word values are just divided by the data size
 - Cross-entropy: https://stackoverflow.com/a/41990932/656740
   - NB entropy is a measure of disorder; a perfect model would have ~ 0 entropy
   - I'm not totally sure Bullet 3 is correct; usually it's p * \log p
 - Perplexity (per word): https://en.wikipedia.org/wiki/Perplexity#Perplexity_per_word

 **Softmax (Slide 13)**

  - Softmax is just a way to turn a vector of _N_ numbers into probabilities
    - https://en.wikipedia.org/wiki/Softmax_function

**Backprop and parameter updates (Slides 16â€“17)**

 - Crescent NN material
 - https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

**Tying Embeddings (Slide 29)**

 - I think his diagram is unhelpful. Not sure it makes sense to tie embeddings 
   if you're stacking the word vectors on top of one another (the 
   dimensionality isn't right). It _does_ make sense if you're summing 
   elementwise.

**Optimizers/Momentum (Slide 31)**

 - Also mentioned in Crescent NN material
 - Also, here's the big list he references: https://ruder.io/optimizing-gradient-descent/index.html#whichoptimizertochoose

**Dropout (Slide 39)**

 - In practice we don't need to do the scaling he mentions by hand. PyTorch, 
   e.g., does it for us when we tell the model whether it's in training or 
   testing mode (`model.train()` and `model.eval()`, respectively)

**Minibatching (Slide 43)**
 
 - Numpy has plenty of detailed info on "broadcasting" that might be helpful: 
   https://numpy.org/devdocs/user/theory.broadcasting.html
 - In the end, we want to do a single operation on a matrix (say) rather than 
   many operations on individual vectors. Often the math works out the same 
   either way, so we use matrices.
